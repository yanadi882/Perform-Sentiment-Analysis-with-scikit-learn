{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='http://www.scienceacademy.ca'> <img style=\"float: left;height:70px\" src=\"Log_SA.jpeg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not installed the required package/library so far:\n",
    "<br>Anaconda users: **conda install nltk=3.2.3**<br>\n",
    "Others: **pip install nltk=3.2.3**<br>\n",
    "*Please use the suggested version to avoid any related issues, once you finish the course, you can always try other versions, they are not very different* <br>\n",
    "**Thanks**\n",
    "\n",
    "# Natural Language Processing (NLP) with Python\n",
    "## (additional topic in the course)\n",
    "\n",
    "\n",
    "Hi Guys, <br>\n",
    "\n",
    "Welcome to the Natural Language Processing with Python. Mystery of Natural Language still needs to be solved. Natural Language Processing (NLP) is challenging and one of the most demanding skills in the field of Data Science. Challenges in NPL frequently involve speech recognition, natural-language understanding, and natural-language generation.<br> \n",
    "\n",
    "We generate data in several ways, speaking, writing, messaging and so many other ways. Majority of the data, we generate, exists in the text format, a highly unstructured data in nature. According to the studies, more tha [80% of the available data is in un-structured form](https://support.sas.com/resources/papers/proceedings14/1288-2014.pdf).<br>\n",
    "\n",
    "&#9758; *<font style=\"font-size:14px;color:green;\">Few very common examples of generating data in our daily life includes: Posts on social media (e.g. FaceBook, Tweeter etc), chat conversations among users, news, blogs, research or any other type of articles, books, product or services reviews, patient records, health-care data and much more. Another recent AI ways include chatbots and voice driven bots. So much unstructured data!!!</font>*\n",
    "\n",
    "\n",
    "Without processing (reading and understanding) text data, we can not present the conclusions or take actions accordingly. Either we have to manually analyze this huge amount of data (which is growing exponentially with time) or we need some kind of automated system to do this data analysis for us. \n",
    "\n",
    "In order to produce actionable insights from text data, it is very important to get familiar with the techniques and principles of Natural Language Processing (NLP). Today, NPL is a big topic and could easily fit into several course in your college life. However, we will try to learn the as much as possible using practical approach with real dataset. <br>\n",
    "\n",
    "\n",
    "In this section, we are going to discuss and overview the basics of Natural Language Processing. This consists of combining machine learning techniques with text, and using maths and statistics to get that text in a format that a machine learning algorithms can understand!<br>\n",
    "\n",
    "Let's work with [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). The dataset is a public set of SMS labeled messages that have been collected for mobile phone spam research. The dataset is a part of [UCI repository](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) and was donated for public use in 2012. This rich dataset contain over 5000 instances and is a great dataset to start with NLP. You can either download the dataset UCI website or use the one which is provided in the course. If you decided to download from the UCI website directly, you will get a folder contain dataset along with readme file, which contain the description on the dataset.<br>   \n",
    "\n",
    "**Required libraries:**<br> \n",
    "In order to work with NLP, we need to have Python's [NLTK (Natural Language Toolkit)](https://www.nltk.org) installed. Instructions are provided in the lecture notes and I hope you have already installed this library. We also need to download the corpus for stopwords to work with this project. <br>\n",
    "\n",
    "[Official documentation for nltk is always a great resource!](http://www.nltk.org)<br>\n",
    "[Working With Text Data--Tutorial on scikit-learn](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "Please note, we will learn several useful concepts and their use in NLP along the way. <br>\n",
    "Let's import nltk to start with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk comes with a collection of packages. Let's download the `stopwords` at the moment. In order to do this, we can use [`download_shell()`](http://www.nltk.org/api/nltk.html?highlight=download_shell#nltk.downloader.download_shell) to open an interactive shell. This allows us to explore the available packages and either we can download all or the one we need!  <br>\n",
    "You will see these options:<br>\n",
    "`d) Download   l) List    u) Update   c) Config   h) Help   q) Quit` <br>\n",
    "to **see the list** of packages, enter **l**. To **download**, enter **d** and then enter the package name, `stopwords` in this case.<br> \n",
    "I have already downloaded the package, it will say `Package stopwords is already up-to-date!`<br>\n",
    "Let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#nltk.download_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the messages look like and read the dataset in `sms` using list comprehension!<br>\n",
    "\n",
    "&#9758; *<font style=\"font-size:14px;color:green;\">Although, we can use use `line.rstrip()` to removes whitespace, newline characters, tab characters, and characters (\\n \\t \\r respectively) from the tail of a line. <b>We don't need it here because we are going to use the power of pandas</b> to read such files rather than parsing them manually</font>*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms = [line for line in open('smsspamcollection/SMSSpamCollection')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many messages we have in the list 'sms'. Let's re-confirm the type of sms as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, list)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sms), type(sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5574 messages in the corpus, we can check any message e.g. for 2 sms[1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tOk lar... Joking wif u oni...\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how the message looks like\n",
    "sms[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see, the file is tab separated `(\\t)`. <br>\n",
    "* The first column will be the label/target \"ham/spam\".\n",
    "* The second column will be the actual message. \n",
    "\n",
    "Let's use the power of pandas to read the text file in a dataframe with ham/spam as target and message as 'sms'.<br>\n",
    "We know the file is tab separated, let pass `sep = '\\t'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pandas first\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                                sms\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading file with sep = '\\t'. We can output the head of our dataframe\n",
    "df = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n",
    "                names=['target', 'sms'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use `info()` and `describe()` methods on our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "target    5572 non-null object\n",
      "sms       5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                     sms\n",
       "count    5572                    5572\n",
       "unique      2                    5169\n",
       "top       ham  Sorry, I'll call later\n",
       "freq     4825                      30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 2 unique labels. There are less unique message as compared to the actual count, this means there are repeated message in the dataset. This makes sense as some repeated messages could be the standard and very common messages e.g. 'Sorry, I'll call later', 'yes', 'no' etc. <br>\n",
    "\n",
    "**Let's use `describe()` on the grouped message as ham or spam. <br>\n",
    "Recall, `groupby()` method. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">sms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sms                                                               \n",
       "       count unique                                                top freq\n",
       "target                                                                     \n",
       "ham     4825   4516                             Sorry, I'll call later   30\n",
       "spam     747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('target').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a little more insight of the dataset. We see, most of the messages are ham, the common one is 'Sorry, I'll call later' whereas the most common spam message is 'Please call our customer service....'.<br> \n",
    "\n",
    "\n",
    "## Feature Engineering\n",
    "In NLP, feature engineering has significant importance, which is usually a large part of processing. Feature engineering involves deep understanding of the field in which you are working, called domain knowledge.  \n",
    "By definition, feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. This is fundamental to the application of machine learning, and is both difficult and expensive.<br>\n",
    "\n",
    "&#9758; <font style=\"font-size:13px;color:green;\">*Feature engineering is another big area of expertise, as a data scientist, you learn to extract features based on your domain knowledge. If you are interested and want to learn more, [Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists](https://www.amazon.com/Feature-Engineering-Machine-Learning-Principles-ebook/dp/B07BNX4MWC) is a good book to read. Wikipedia's article on [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering) also provide a good summary on this topic.*</font>\n",
    "\n",
    "One of the obvious thing is the length of the message in this dataset. We can create this feature and add a new column into our dataframe with the length of each message. We can then see if there is any trend or correlation between length of the message and its class \"ham/spam\".<br>\n",
    "\n",
    "Let's create a column length!<br>\n",
    "*Recall `.apply()`method! We can use this along with `len()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>sms</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                                sms  length\n",
       "0    ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1    ham                      Ok lar... Joking wif u oni...      29\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3    ham  U dun say so early hor... U c already then say...      49\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['sms'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization \n",
    "Let's do some imports and visualize the data. We may get some more information from the data visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use pandas built-in data visualization capabilities here\n",
    "# df['length'].plot(bins=70, kind='hist') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x112641898>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEFCAYAAAAPCDf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfRGRCFHoi4hEiEJfRCRCFPoiIhHy/wHsbOEHmg+YEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1122edda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's use sns distplot here \n",
    "sns.distplot(df['length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see a bimodal distribution in the length column. Looks like, the choice of the length of the message during feature engineering was a good option.<br>\n",
    "Let's explore more, we can plot ham and spam separately! -- we can use pandas data visualization capabilities as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAADWCAYAAACOh8VmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1115a6978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(column='length', by='target', bins=70,figsize=(10,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we see that there is a general trend in ham and spam messages. Generally, ham messages are under 200 characters! <br>\n",
    "Looks like, our newly created feature worked very good and suggest that we can separate the ham vs spam messages based on this feature!\n",
    "\n",
    "## Text Preprocessing \n",
    "\n",
    "This is a classification problem in which we are trying to predict two classes, ham or spam. In order to train our Machine learning model, we need to convert all the text strings into some kind of numerical data. We have learned the concepts behind such conversion in the lecture. Its time to apply that knowledge. <br>\n",
    "\n",
    "Recall, the bag-of-words (BoW) approach, where each word in the text is represented by a number. <br>\n",
    "Let's convert the raw strings of our messages -- sequence of characters, into vectors -- sequences of numbers.<br>\n",
    "* First thing to do is, to get rid of punctuations in the messages. *Recall basic String methods in Python - early lectures*\n",
    "* Second, convert the messages into their individual words and return a list.\n",
    "* Third, we will remove very common words ('the', 'a', 'is'... etc) from the list of words of the messages. These are stopwords! --- *good to know, most of the search engines are programmed to ignore stopwords!*\n",
    "\n",
    "To remove punctuations, we can use Python's built-in string library `string.punctuation` to get a quick list of all the possible punctuation .<br>\n",
    "\n",
    "Let's see how this all will work on a **sample string first!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do the import\n",
    "import string\n",
    "\n",
    "# create a sample with punctuations\n",
    "s = 'is this a sample message! if yes, remove punctuations.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what string.punctuation is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuations.\n",
    "Check characters if there are punctuations in 's' and get a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 's', ' ', 't', 'h', 'i', 's', ' ', 'a', ' ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc = [char for char in s if char not in string.punctuation]\n",
    "nopunc[0:10] # you can print complete list, its long!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now. let's join the characters again at '' (white space) to form the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is this a sample message if yes remove punctuations'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc = ''.join(nopunc)\n",
    "nopunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop-words.\n",
    "Now let's see how to remove **`stopwords`**. Remember, we installed `stopwords` package in the beginning. We need to import that package and apply on `nopunc` to remove the **English stopwords**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the import for stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# Just to see first 5 English stopwords\n",
    "stopwords.words('english')[0:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split `nopunc` into its words, deal with the lower and upper case and get a list of those which are not in `stopwords.words`.<br>\n",
    "We can use list comprehension here. *Recall the early lectures in Python Essentials!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input was:  ['is', 'this', 'a', 'sample', 'message', 'if', 'yes', 'remove', 'punctuations']\n",
      "Output is:  ['sample', 'message', 'yes', 'remove', 'punctuations']\n"
     ]
    }
   ],
   "source": [
    "# removing all stopwords\n",
    "clean_s = [word for word in nopunc.split() \n",
    "              if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "#Let's compare the input anf output \n",
    "print('Input was: ',nopunc.split())\n",
    "print('Output is: ',clean_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have worked with the sample sentence, **Let's apply all the above steps to our dataframe of messages, the simple way is to create a function with all the steps and use `apply()` to do the processing**<br>\n",
    "We will write a function `\"process_text(raw_text)` and pass in the raw text. The function will take the string of raw text and perform the following:\n",
    "* Remove all punctuation\n",
    "* Remove all stopwords\n",
    "* Returns a list of the cleaned text\n",
    "\n",
    "Let's write a function to do the above steps, we will use this function soon.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(raw_text):\n",
    "    # Check for the punctuations \n",
    "    nopunc = [char for char in raw_text \n",
    "              if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters \n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Remove stopwords (if any)\n",
    "    return [word for word in nopunc.split() \n",
    "            if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "For a given character sequence and a defined document unit, **tokenization** is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. <br>\n",
    "For example:<br>\n",
    "\n",
    "* **Input:** Friends, Romans, Countrymen, lend me your ears; \n",
    "* **Output:** Friends Romans Countrymen lend me your ears\n",
    "\n",
    "These tokens are often loosely referred to as terms or words, but it is sometimes important to make a type/token distinction.<br>\n",
    "\n",
    "Let's do the Tokenization Now! We can grab few messages from the dataframe and see how the function `process_text(raw_text)` works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "Name: sms, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before tokenization\n",
    "df['sms'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "Name: sms, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After tokenization\n",
    "df['sms'].head(2).apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the stopwords are removed and we got the list of tokens that are actually the words we want.<br>\n",
    "**Before we move on, let's pause and learn another useful concepts in Text Normalization.**<br>\n",
    "\n",
    "There are lots of ways to normalize the text data. In this example, we have used a simple text normalization in our text pre-processing. NLTK library has a good number of built-in tools {*[Check the documentation on nltk official website](http://www.nltk.org/book/ch03.html)*} and very helpful documentation on other method of text normalization. <br>\n",
    "The key concepts in NLP, including **tokenization and stemming**.<br> \n",
    "*<b>Stemming</b> involves the extraction of stem words in the text normalization, for example, if we have similar words in our text, such as, `[play, playing, played]`, the only useful word is play, stemming tries to return only `[play]`.* <br>\n",
    "\n",
    "In our dataset, stemming may not be useful because of the presence of several shorthand words `[U, dun, 2, u, c, Nah, .....]` which is common in the text messages. Stemming may not be very helpful in our particular dataset, however, it works greatly in several cases such as news data, books, research articles etc. <br>\n",
    "\n",
    "&#9758; *Another important thing is dealing with currency symbols and other type of regular expressions. Please consult the documentation of [Processing Raw Text](http://www.nltk.org/book/ch03.html) to learn more about this.*<br><br><br>\n",
    "Let's apply `process_text()` to the `sms` column in our dataframe to get the **tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenization on df['sms']\n",
    "df['sms'].apply(process_text);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "We have created a method to get the lists of tokens. As already mentioned, in order to do Machine Learning using scikit-learn, we need to convert each message into a vector form, so that the Machine Learning model can understand.\n",
    "\n",
    "Recall the **Bag of words (BoW)** technique from the lecture notes!<br>\n",
    "We will do the following steps using BoW model now.\n",
    "\n",
    "1. **Term Frequency**, by counting how many times does a word appeared in each message.\n",
    "\n",
    "2. **Inverse Document Frequency**, which is actually a weigh the counts *(recall lecture notes)*, frequent tokens get lower weight.\n",
    "\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "\n",
    "Recall from the lecture notes, how the first step will look like after Term Frequency (the values / numbers in the table below):\n",
    "\n",
    "<table border = “1“>\n",
    "<tr>\n",
    "<th></th> <th>SMS_1</th> <th>SMS_2</th> <th>...</th> <th>SMS_n</th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word_1</b></td><td>3</td><td>0</td><td>...</td><td>5</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word_2</b></td><td>2</td><td>0</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>...</b></td> <td>...</td><td>...</td><td>...</td><td>...</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word_n</b></td> <td>0</td><td>0</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "Each vector have as many dimensions as there are unique words in the SMS corpus.<br>\n",
    "scikit-learn have a built-in tool, [**CountVectorizer**](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) which convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "Let me introduce another term here:<br>\n",
    "[**Sparse Matrix**](https://en.wikipedia.org/wiki/Sparse_matrix): In numerical analysis and computer science, a sparse matrix or sparse array is a matrix in which most of the elements are zero.<br>\n",
    "\n",
    "*&#9758; Imagine, how many zeros you should expect in you matrix of token counts?<br>\n",
    "Lots!, since, there are so many messages, so many words in the BoW and lots missing in each message! right? You will get a sparse matrix! <br>*\n",
    "\n",
    "Anyhow, let's move on and import `CountVectorizer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9758; `CountVectorizer()` and `shit+tab` for the doc string<br>\n",
    "\n",
    "We can pass a range of parameters to the `CountVectorizer`. In this case, we are going to pass `analyzer = process_text`, which is our own created function.<br>\n",
    "In the output, we will get a **BoW** (e.g. we call it 'bow_transformer' here) after processing text according to the analyzer. The text data is large and processing might get some time, depends upon your computer etc. We are creating a very large matrix, all the rows are words and  <br>\n",
    "Let's fit the `CountVectorizer()` to `sms` column of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(\n",
    "    analyzer=process_text).fit(df['sms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "# How many words we got in the vocabulary?\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10979"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(bow_transformer.vocabulary_)#['way'] #to get the word's id \n",
    "bow_transformer.vocabulary_.get('way') #to get the word's id / index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better understanding of the working process, I think, its a good idea to transform a single sms to its bow count as a vector first. We need to call `.transform` on the bow_transformer and pass in the selected sms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1914)\t1\n",
      "  (0, 3964)\t1\n",
      "  (0, 4880)\t1\n",
      "  (0, 6517)\t1\n",
      "  (0, 6907)\t1\n",
      "  (0, 10405)\t1\n",
      "  (0, 10698)\t1\n",
      "  (0, 10979)\t2\n",
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "# grabbing message at index 18 -- 19th message (index starts at 0)\n",
    "sms_18 = df['sms'][18]\n",
    "\n",
    "#Transforming sms_18 to its bow (bow_18)\n",
    "bow_18 = bow_transformer.transform([sms_18])\n",
    "\n",
    "# how the bow_18 look like and what is its shape, let's check \n",
    "print(bow_18)\n",
    "print(bow_18.shape) #(n_rows, n_cols) -- shape is 1 by our entire vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#type(bow_18)\n",
    "#print(bow_18.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 unique words in the selected sms (after removing common stop words). Once is appeared twice, let's check the one which appeared twice in the sms! <br>\n",
    "From the `bow_transformer` we can call the method, `get_feature_names()`, and ask for the for the repeated word, its index 10979.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[10979])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the word \"**way**\" appeared twice. <br><br>\n",
    "Now, Let's move on and use `.transform` with `bow_transformer` and transform the entire DataFrame of sms corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bow = bow_transformer.transform(df['sms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the shape of our df_bow matrix (matrix of token counts) and also check how many of its element are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix of token counts(sms_bow):  (5572, 11425)\n",
      "No of Non-Zero elements:  50548\n"
     ]
    }
   ],
   "source": [
    "print('Shape of matrix of token counts(sms_bow): ', df_bow.shape) # (n_rows, n_cols)\n",
    "print('No of Non-Zero elements: ', df_bow.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the sparsity of the matrix using the formula below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity of df_bow matrix: 0.07940295412668218\n"
     ]
    }
   ],
   "source": [
    "total_no_of_elements = df_bow.shape[0] * df_bow.shape[1]\n",
    "sparsity = (df_bow.nnz / total_no_of_elements)*100\n",
    "print('sparsity of df_bow matrix: {}' .format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excellent!**<br>\n",
    "* **We are done with term frequency.** How many times a word/term appeared in a sms e.g. way appeared 2 times in sms_18<br>\n",
    "* **Its time to compute for term weighting and normalization using TF-IDF (reference to the lecture notes with practical examples).** <br>\n",
    "\n",
    "scikit-learn provides a tool `TfidfTransformer`, let's do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing import \n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create an instance for TfidfTransformer and fit to the df_bow\n",
    "tfidf_trans = TfidfTransformer()\n",
    "tfidf_trans.fit(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously done, for better understanding, **lets work with a single message \"sms_18\" and transform \"bow_18\" to tfidf.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10979)\t0.461428791272\n",
      "  (0, 10698)\t0.14951677306\n",
      "  (0, 10405)\t0.407177880884\n",
      "  (0, 6907)\t0.407177880884\n",
      "  (0, 6517)\t0.257793651128\n",
      "  (0, 4880)\t0.259547084742\n",
      "  (0, 3964)\t0.407177880884\n",
      "  (0, 1914)\t0.365410001719\n"
     ]
    }
   ],
   "source": [
    "tfidf_18 = tfidf_trans.transform(bow_18)\n",
    "#lets print TF-IDF for the message 18\n",
    "print(tfidf_18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the weight values of each of the word in sms_18, the relationship between TF and IDF.<br>\n",
    "We can check IDF for any single random word as well, let's try the one which came twice in sms_18 \"**way**\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for \"Way\" is:  5.0613405961\n"
     ]
    }
   ],
   "source": [
    "print('IDF for \"Way\" is: ',tfidf_trans.idf_[bow_transformer.vocabulary_['way']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we got better idea on weighting, lets transfer our entire BoW \"sms_bow\" to TF-IDF corpus! <br>\n",
    "*Instead of passing a single sms as a BoW, we will pass the entire corpus.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tfidf = tfidf_trans.transform(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 11425)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the shape of sms_tfidf\n",
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning -- Training and evaluating the model  \n",
    "Now, as we have transformed our data into its vector form, we can train our Machine Learning algorithm.<br>\n",
    "Its a spam/ham detection, a classification problem. We have already learned several models for classification problem and use any one of them. However, [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#cite_note-rennie-2) is considered as a good choice in text retrieval community. With appropriate pre-processing, Naive Bayes is competitive in this domain with more advanced methods including support vector machines, [Reference link](http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf). <br>\n",
    "Let's use the one which is accepted by the experts as a better choice!\n",
    "and import multinomial Naive Bayes model, [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's do the import for Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating instance\n",
    "spam_ham_detection = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split \n",
    "X = df_tfidf\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training on train data\n",
    "spam_ham_detection.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**<br>\n",
    "This is good, we got the model which is doing predictions for ham/spam messages!<br>\n",
    "Let's try the model on entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions for the test data\n",
    "pred = spam_ham_detection.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1593    0]\n",
      " [  71  175]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      1.00      0.98      1593\n",
      "       spam       1.00      0.71      0.83       246\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print (confusion_matrix(y_test, pred))\n",
    "print (classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Machine Learning section we have used vectorized form of our dataframe `df_tfidf` while split. **This is not what we usually do.** We did this here, becasue we want to follow the flow and for learning and training purpose. Let me introduce a great feature from scikit-learn, `Pipeline` capabilities to store a pipeline of workflow.<br>\n",
    "## [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) \n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. We can set up all the transformation, we did during the text processing, in a single unit using pipeline feature of scikit-learn. Rather than doing all steps one-by-one, we can then call that single unit for our data processing. In this way, we save lots of time and there is no need to re-do all the transformation steps manually. A simple call of pipeline object, with stored steps, on the data will do all the processing in future.  <br>\n",
    "\n",
    "Let's see this will work on our dataset!\n",
    "Let's do the **`train_test_split()`** again using our raw data. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_test_split() on the raw data\n",
    "X = df['sms']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pipeline, need to do import first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    # Tokenization using scikit's CountVectorizer \n",
    "    ('baw', CountVectorizer(analyzer=process_text)),  \n",
    "    \n",
    "    # Computing TF-IDF  -- weighted scores\n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    \n",
    "     # Naive Bayes classifier to train on TF-IDF vectors\n",
    "    ('model_nb', MultinomialNB()), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traing / Fitting using pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('baw', CountVectorizer(analyzer=<function process_text at 0x112954400>, binary=False,\n",
       "        decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,..._tf=False, use_idf=True)), ('model_nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions for the test data\n",
    "pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1593    0]\n",
      " [  65  181]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      1.00      0.98      1593\n",
      "       spam       1.00      0.74      0.85       246\n",
      "\n",
      "avg / total       0.97      0.96      0.96      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (confusion_matrix(y_test, pred))\n",
    "print (classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Excellent work!\n",
    "I know this was long. I hope you enjoyed working with the real dataset. NPL is a great skill to add in your profile. It is kind of a sub-field of Machine Learning and need practice along with the domain knowledge. <br>\n",
    "\n",
    "### To Do!\n",
    "Try using other models such as SVM or Random Forests instead of Naive Bayes and compare your results. It is easy, you only need to change the classifier in the pipeline and re-run. <br>\n",
    "Please do this yourself first. I have already tried Logistic Regression and Random Forests for you, in case you need help. You will see how useful this pipeline feature is! <br>\n",
    "In the real world data, you will set-up the transformations and create such pipeline to make things easier and try different options!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1589    4]\n",
      " [  91  155]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.95      1.00      0.97      1593\n",
      "       spam       0.97      0.63      0.77       246\n",
      "\n",
      "avg / total       0.95      0.95      0.94      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('baw', CountVectorizer(analyzer=process_text)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    \n",
    "     # only change is RandomForestClassifier() \n",
    "    ('model', LogisticRegression()), \n",
    "])\n",
    "# training the model\n",
    "pipeline.fit(X_train,y_train)\n",
    "# doing predictions\n",
    "pred = pipeline.predict(X_test)\n",
    "#Evaluation\n",
    "print (confusion_matrix(y_test, pred))\n",
    "print (classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1592    1]\n",
      " [  64  182]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      1.00      0.98      1593\n",
      "       spam       0.99      0.74      0.85       246\n",
      "\n",
      "avg / total       0.97      0.96      0.96      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('baw', CountVectorizer(analyzer=process_text)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    \n",
    "     # only change is RandomForestClassifier() \n",
    "    ('model', RandomForestClassifier()), \n",
    "])\n",
    "\n",
    "# training the model\n",
    "pipeline.fit(X_train,y_train)\n",
    "# doing predictions\n",
    "pred = pipeline.predict(X_test)\n",
    "#Evaluation\n",
    "print (confusion_matrix(y_test, pred))\n",
    "print (classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just a brush-up reminder, you may want to give a quick look at:**<br>\n",
    "* **[`precision and recall`](https://en.wikipedia.org/wiki/Precision_and_recall)**\n",
    "* **[`f1-score`](https://en.wikipedia.org/wiki/F1_score)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
